{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements: wheels,dlib,face-recognition,mutils,imageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "person  :  96.71110510826111\n",
      "person  :  89.26351070404053\n",
      "person  :  78.37235927581787\n",
      "person  :  76.80343985557556\n",
      "person  :  70.4874336719513\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath('/Users/peyamowar/Downloads/clickbait_youtube/resnet50_coco_best_v2.1.0.h5')\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image='/Users/peyamowar/Downloads/clickbait_youtube/images/109.jpg', output_image_path='/Users/peyamowar/Downloads/clickbait_youtube/images/new_image.jpg')\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New directory created\n",
      "Extracted 5 faces from all images\n"
     ]
    }
   ],
   "source": [
    "#will extract all faces with 50%<confidence from all images in \"/images\" folder and store only\n",
    "#the cropped face in \"/faces\" folder with name \"<face number>_<original image name>\"\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "prototxt_path = '/Users/peyamowar/Downloads/clickbait_youtube/model_data/deploy.prototxt'\n",
    "caffemodel_path = '/Users/peyamowar/Downloads/clickbait_youtube/model_data/weights.caffemodel'\n",
    "\n",
    "# Read the model\n",
    "model = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
    "\n",
    "# Create directory 'faces' if it does not exist\n",
    "if not os.path.exists('faces'):\n",
    "\tprint(\"New directory created\")\n",
    "\tos.makedirs('faces')\n",
    "\n",
    "# Loop through all images and strip out faces\n",
    "count = 0\n",
    "for file in os.listdir('/Users/peyamowar/Downloads/clickbait_youtube/images/'):\n",
    "\tfile_name, file_extension = os.path.splitext(file)\n",
    "\tif (file_extension in ['.png','.jpg']):\n",
    "\t\timage = cv2.imread('/Users/peyamowar/Downloads/clickbait_youtube/images/' + file)\n",
    "\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\tblob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "\t\tmodel.setInput(blob)\n",
    "\t\tdetections = model.forward()\n",
    "\n",
    "\t\t# Identify each face\n",
    "\t\tfor i in range(0, detections.shape[2]):\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t\t# If confidence > 0.5, save it as a separate file\n",
    "\t\t\tif (confidence > 0.5):\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\t\tframe = image[startY:endY, startX:endX]\n",
    "\t\t\t\tcv2.imwrite('/Users/peyamowar/Downloads/clickbait_youtube/' + '/faces/' + str(i) + '_' + file, frame)\n",
    "\n",
    "print(\"Extracted \" + str(count) + \" faces from all images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#will take folder of faces to recognise stored in '/faces' folder and create their embeddings\n",
    "#and save them in \"face_enc\" file. will then read a specified image and identify\n",
    "#all detected faces(from those stored in the faces folder) and print the names as \"name of the face file in the faces folder\"\n",
    "\n",
    "\n",
    "import face_recognition\n",
    "import imutils\n",
    "from imutils import paths\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    " \n",
    "#get paths of each file in folder named Images\n",
    "#Images here contains my data(folders of various persons)\n",
    "imagePaths = list(paths.list_images('faces'))\n",
    "knownEncodings = []\n",
    "knownNames = []\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    # load the input image and convert it from BGR (OpenCV ordering)\n",
    "    # to dlib ordering (RGB)\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #Use Face_recognition to locate faces\n",
    "    boxes = face_recognition.face_locations(rgb,model='hog')\n",
    "    # compute the facial embedding for the face\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    # loop over the encodings\n",
    "    for encoding in encodings:\n",
    "        knownEncodings.append(encoding)\n",
    "        knownNames.append(name)\n",
    "#save emcodings along with their names in dictionary data\n",
    "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "#use pickle to save data into a file for later use\n",
    "f = open(\"face_enc\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n",
    "\n",
    "#find path of xml file containing haarcascade file\n",
    "cascPathface = os.path.dirname(cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "# load the harcaascade in the cascade classifier\n",
    "faceCascade = cv2.CascadeClassifier(cascPathface)\n",
    "# load the known faces and embeddings saved in last file\n",
    "data = pickle.loads(open('face_enc', \"rb\").read())\n",
    "#Find path to the image you want to detect face and pass it here\n",
    "image = cv2.imread('images/109.jpg')\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#convert image to Greyscale for haarcascade\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(gray,\n",
    "                                     scaleFactor=1.1,\n",
    "                                     minNeighbors=5,\n",
    "                                     minSize=(60, 60),\n",
    "                                     flags=cv2.CASCADE_SCALE_IMAGE)\n",
    " \n",
    "# the facial embeddings for face in input\n",
    "encodings = face_recognition.face_encodings(rgb)\n",
    "names = []\n",
    "# loop over the facial embeddings incase\n",
    "# we have multiple embeddings for multiple fcaes\n",
    "for encoding in encodings:\n",
    "    #Compare encodings with encodings in data[\"encodings\"]\n",
    "    #Matches contain array with boolean values and True for the embeddings it matches closely\n",
    "    #and False for rest\n",
    "    matches = face_recognition.compare_faces(data[\"encodings\"],\n",
    "    encoding)\n",
    "    #set name =inknown if no encoding matches\n",
    "    name = \"Unknown\"\n",
    "    # check to see if we have found a match\n",
    "    if True in matches:\n",
    "        print(\"hello\")\n",
    "        #Find positions at which we get True and store them\n",
    "        matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "        counts = {}\n",
    "        # loop over the matched indexes and maintain a count for\n",
    "        # each recognized face face\n",
    "        for i in matchedIdxs:\n",
    "            #Check the names at respective indexes we stored in matchedIdxs\n",
    "            name = data[\"names\"][i]\n",
    "            #increase count for the name we got\n",
    "            counts[name] = counts.get(name, 0) + 1\n",
    "            #set name which has highest count\n",
    "            name = max(counts, key=counts.get)\n",
    " \n",
    " \n",
    "        # update the list of names\n",
    "        names.append(name)\n",
    "        # loop over the recognized faces\n",
    "        for ((x, y, w, h), name) in zip(faces, names):\n",
    "            # rescale the face coordinates\n",
    "            # draw the predicted face name on the image\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(image, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "             0.75, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", image)\n",
    "    cv2.waitKey(0)\n",
    "print(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between the two images:  [17064.225]\n"
     ]
    }
   ],
   "source": [
    "#calculates euclidean distance between two images as a measure of similarity\n",
    "\n",
    "import cv2 \n",
    "\n",
    "\t\n",
    "# test image \n",
    "image = cv2.imread('image.jpg') \n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "histogram = cv2.calcHist([gray_image], [0], \n",
    "\t\t\t\t\t\tNone, [256], [0, 256]) \n",
    "\n",
    "# data1 image \n",
    "image = cv2.imread('face to detect image in.jpg') \n",
    "gray_image1 = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "histogram1 = cv2.calcHist([gray_image1], [0], \n",
    "\t\t\t\t\t\tNone, [256], [0, 256])  \n",
    "\n",
    "\n",
    "c1, c2 = 0, 0\n",
    "\n",
    "# Euclidean Distace between data1 and test \n",
    "i = 0\n",
    "while i<len(histogram) and i<len(histogram1): \n",
    "\tc1+=(histogram[i]-histogram1[i])**2\n",
    "\ti+= 1\n",
    "c1 = c1**(1 / 2) \n",
    "\n",
    "print(\"distance between the two images: \",c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-oduouqig\\opencv\\modules\\core\\src\\arithm.cpp:669: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c4f0f12c2738>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mduplicate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The images have same size and channels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdifference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduplicate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-oduouqig\\opencv\\modules\\core\\src\\arithm.cpp:669: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n"
     ]
    }
   ],
   "source": [
    "#IDIOT DOESNT WORK\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "original = cv2.imread('image.jpg')\n",
    "duplicate = cv2.imread('face to detect image in.jpg')\n",
    "\n",
    "# 1) Check if 2 images are equals\n",
    "if original.shape == duplicate.shape:\n",
    "    print('The images have same size and channels')\n",
    "difference = cv2.subtract(original, duplicate)\n",
    "b, g, r = cv2.split(difference)\n",
    "\n",
    "if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:\n",
    "    print('The images are completely Equal')\n",
    "\n",
    "# 2) Check for similarities between the 2 images\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp_1, desc_1 = sift.detectAndCompute(original, None)\n",
    "kp_2, desc_2 = sift.detectAndCompute(image_to_compare, None)\n",
    "\n",
    "index_params = dict(algorithm=0, trees=5)\n",
    "search_params = dict()\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(desc_1, desc_2, k=2)\n",
    "\n",
    "good_points = []\n",
    "ratio = 0.6\n",
    "for m, n in matches:\n",
    "    if m.distance < ratio*n.distance:\n",
    "        good_points.append(m)\n",
    "        print(len(good_points))\n",
    "\n",
    "result = cv2.drawMatches(original, kp_1, image_to_compare, kp_2, good_points, None)\n",
    "\n",
    "# Define how similar they are\n",
    "number_keypoints = 0\n",
    "if len(kp_1) <= len(kp_2):\n",
    "    number_keypoints = len(kp_1)\n",
    "else:\n",
    "    number_keypoints = len(kp_2)\n",
    "print(\"Keypoints 1ST Image: \" + str(len(kp_1)))\n",
    "print(\"Keypoints 2ND Image: \" + str(len(kp_2)))\n",
    "\n",
    "print(\"GOOD Matches:\", len(good_points))\n",
    "print(\"How good it's the match: \", len(good_points) / number_keypoints * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
